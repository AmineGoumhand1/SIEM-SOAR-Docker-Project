version: "3.7"

services:
  thehive:
    image: strangebee/thehive:5.2
    restart: unless-stopped
    depends_on:
      - cassandra
      - elasticsearch
      - minio
      - cortex.local
    mem_limit: 1500m
    ports:
      - "0.0.0.0:9000:9000"
    environment:
      - JVM_OPTS=-Xms1024M -Xmx1024M
      - CASSANDRA_CONTACT_POINT=cassandra
      - INITIAL_ADMIN_USERNAME=admin
      - INITIAL_ADMIN_PASSWORD=secret
      - INITIAL_ADMIN_EMAIL=admin@thehive.local
    command:
      - --secret
      - "Awd33_Yss3r"
      - "--cql-hostnames"
      - "cassandra"
      - "--index-backend"
      - "elasticsearch"
      - "--es-hostnames"
      - "elasticsearch"
      - "--s3-endpoint"
      - "http://minio:9002" # THis is a server that let theHive use S3 storage, here we use minio service, to store like large files, reports,..
      - "--s3-access-key"
      - "minioadmin"
      - "--cortex-port"
      - "9001"
      #- "--cortex-keys"
      #- "7BP6xZi33wlqxogECK6xgElzcQnMR7Wa"
    volumes:
      - ./thehive/conf/application.conf:/etc/thehive/application.conf 
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.2
 
  
  elastalert:
    image: jertel/elastalert2:latest
    restart: unless-stopped
    depends_on:
      - elasticsearch
      - thehive
    volumes:
      - ./elastalert/config.yaml:/opt/elastalert/config.yaml
      - ./elastalert/rules:/opt/elastalert/rules
    environment:
      - TZ=Europe/London
    ports:
      - "3030:3030"
    command: ["--verbose"]

    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.3

  cassandra:
    image: cassandra:4
    restart: unless-stopped
    ports:
      - "0.0.0.0:9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=hive
    volumes:
      - cassandradata:/var/lib/cassandra
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.4

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.9
    restart: unless-stopped
    mem_limit: 1g
    ports:
      - "0.0.0.0:9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - cluster.name=hive
      - http.host=0.0.0.0
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    volumes:
      - elasticsearchdata:/usr/share/elasticsearch/data
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.5

  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.9
    depends_on:
      - elasticsearch
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    environment:
      - LS_JAVA_OPTS=-Xmx256m -Xms256m
      - ELASTICSEARCH_HOST_PORT=http://elasticsearch:9200
      - ELASTIC_USERNAME=elastic
      - ELASTIC_PASSWORD=changeme
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.6

  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.0
    depends_on:
      - elasticsearch
    ports:
      - "0.0.0.0:5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    volumes:
      #- ./plugins/elastalertKibanaPlugin-1.7.2-8.16.0.zip:/usr/share/kibana/plugins/elastalertKibanaPlugin-1.7.2-8.16.0.zip
      - ./kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml
    # Adding a command to install the ElastAlert plugin for Kibana if it is not already installed
    # command:
    #   - "bash"
    #   - "-c"
    #   - |
    #   if [ ! -d /usr/share/kibana/plugins/elastalertKibanaPlugin ]; then
    #     echo 'Installing ElastAlert plugin...';
    #     bin/kibana-plugin install file:///usr/share/kibana/plugins/elastalertKibanaPlugin-1.7.2-8.16.0.zip;
    #   fi;
    #   /usr/local/bin/kibana-docker
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.7

  minio:
    image: quay.io/minio/minio
    restart: unless-stopped
    command: ["minio", "server", "/data", "--console-address", ":9002"]
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "0.0.0.0:9002:9002"
    volumes:
      - miniodata:/data
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.8
# In cortext, we need after each fully redoployement to delete the cortex index in elasticsearch with : curl -X DELETE "http://localhost:9200/cortex*"
# created user API KEY : key_here for user :  cortex_user@gmail.com and pass : Abcd@12345678 and admin user : admin@thehive.local with pass : Abcd@12345678
  cortex.local:
    image: thehiveproject/cortex:3.1.3-1
    restart: unless-stopped
    environment:
      - job_directory=/tmp/cortex-jobs
      - docker_job_directory=/tmp/cortex-jobs
  
    volumes:
      #For analyzers and responders (called neurons, also based on docker containers) to work, we need to bind the hosts docker socket into the cortex container
      #so it can use the docker service of the host, and share the job directory between the container and the host.
      #An alternative way of doing this would be to run docker (neurons) within the cortex docker container (docker-ception), the container will need to be run in 
      #privileged mode and you will need the --start-docker parameter for this work. It is however not advised to run docker containers in priviliged mode because it
      #grants the docker container root capabilities over the host system which is a security risk. 
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/cortex-jobs:/tmp/cortex-jobs
      - ./cortex/logs:/var/log/cortex
      - ./cortex/application.conf:/cortex/application.conf
    depends_on:
      - elasticsearch
    ports:
      - "0.0.0.0:9001:9001"
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.9


# New added user API KEY : bCKHxoDolUWMvVLYKlLhqZTQ3juiFErlvLWXmOXD
  misp.local:
    image: coolacid/misp-docker:core-latest
    restart: unless-stopped
    depends_on: 
      - misp_mysql
    ports:
      - "0.0.0.0:80:80"
      - "0.0.0.0:443:443"
    volumes:
      - "./server-configs/:/var/www/MISP/app/Config/"
      - "./logs/:/var/www/MISP/app/tmp/logs/"
      - "./files/:/var/www/MISP/app/files"
      - "./ssl/:/etc/nginx/certs"
    extra_hosts:
      - "misp.local:10.0.2.10"
    environment:
      - MYSQL_HOST=misp_mysql
      - MYSQL_DATABASE=mispdb
      - MYSQL_USER=mispuser
      - MYSQL_PASSWORD=misppass
      - MISP_ADMIN_EMAIL=admin@admin.test
      - MISP_ADMIN_PASSPHRASE=admin
      - MISP_BASEURL=http://10.0.2.16 # to surpass the css loading prob, add this MISP_BASEURL variable with the IP of the misp server, then update /etc/hosts file in your local pc with misp.local to that IP
      - TIMEZONE=Europe/London
      - "BASEURL=http://misp.local" 
      - "INIT=true"         
      - "CRON_USER_ID=1"   
      - "REDIS_FQDN=redis"
      - BASEURL=https://misp.local

    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.16

  misp_mysql:
    image: mysql/mysql-server:5.7
    restart: unless-stopped
    volumes:
      - mispsqldata:/var/lib/mysql
    environment:
      - MYSQL_DATABASE=mispdb
      - MYSQL_USER=mispuser
      - MYSQL_PASSWORD=misppass
      - MYSQL_ROOT_PASSWORD=mispass
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.11


  redis:
    image: redis:latest
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.12

  misp-modules:
    image: coolacid/misp-docker:modules-latest
    depends_on:
      - redis
      - misp_mysql
    environment:
      - REDIS_BACKEND=redis
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.13

  portainer:
    image: portainer/portainer-ce:latest
    restart: unless-stopped
    ports:
      - "9443:9443"
      - "8000:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.14

  node-red:
    image: nodered/node-red:latest
    restart: unless-stopped
    ports:
      - "1880:1880"
    volumes:
      - nodered_data:/data
    networks:
      MySIEM-Network:
        ipv4_address: 10.0.2.15

volumes:
  miniodata:
  cassandradata:
  elasticsearchdata:
  thehivedata:
  mispsqldata:
  cortexdata:
  portainer_data:
  nodered_data:
  shuffle-apps:
  shuffle-files:
  shuffle-database:

networks:
  MySIEM-Network:
    name: MySIEM-Network
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.2.0/24
